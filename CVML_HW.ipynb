{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVML_HW.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GEjhEKiGZuk1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWFVscRmZmVT",
        "colab_type": "text"
      },
      "source": [
        "# Homework\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TesXqCt7coAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update torch, torchvision and numpy\n",
        "!pip install -U torch torchvision numpy opencv-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdmC5juWvsMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Homework dataset\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/HW.zip\n",
        "!unzip -qq HW.zip\n",
        "!rm HW.zip\n",
        "\n",
        "# Traffic Sign Classification set\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/trafficSignsHW.zip\n",
        "!unzip -qq trafficSignsHW.zip\n",
        "!rm trafficSignsHW.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ynvKXBOwGB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install homework repository\n",
        "# !git clone https://github.com/szykry/CVML_HW.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Bypy08dDaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set root folder\n",
        "import os\n",
        "name = \"/content/CVML_HW/\"\n",
        "os.chdir(name)\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWbWDlg-v5oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation\n",
        "from HW.evaluate import evaluate\n",
        "\n",
        "#file = open('HW/annotations.pickle','rb')\n",
        "#predictions = pickle.load(file)\n",
        "\n",
        "evaluate(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEjhEKiGZuk1",
        "colab_type": "text"
      },
      "source": [
        "# Tradition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4HMK6T7KVtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://docs.opencv.org/4.3.0/d1/d1a/namespacecv_1_1cuda.html\n",
        "\n",
        "# Tábla -> hough\n",
        "# szegmentális\n",
        "# cv.threshold\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvzlqdEVKl39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OpenCV\n",
        "import cv2\n",
        "\n",
        "#Numpy - numeric library\n",
        "import numpy as np\n",
        "\n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#This way it doesn't try to open a window un the GUI - works in python notebook\n",
        "%matplotlib inline\n",
        "\n",
        "minVal=0.0\n",
        "maxVal=0.0\n",
        "\n",
        "img = cv2.imread(\"HW/g3/rgb/1.jpg\")   # 2.param: RGB, Grayscale, Bináriskép\n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "minVal, maxVal, _, _ = cv2.minMaxLoc(img_gray)    # hol vannak a szélsőértékek\n",
        "res=img_gray\n",
        "cv2.convertScaleAbs(img_gray,res,255/(maxVal-minVal),-minVal)  # affin -> kép*alfa + béta\n",
        "\n",
        "depth_img = cv2.imread(\"HW/g3/depth/1.png\")\n",
        "depth_img_gray = cv2.cvtColor(depth_img, cv2.COLOR_BGR2GRAY)\n",
        "minVal, maxVal, _, _ = cv2.minMaxLoc(depth_img_gray)    # hol vannak a szélsőértékek\n",
        "depth_res=depth_img_gray\n",
        "cv2.convertScaleAbs(depth_img_gray,depth_res,255/(maxVal-minVal),-minVal)  # affin -> kép*alfa + béta\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(res,cmap='gray')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(depth_res,cmap='gray')\n",
        "\n",
        "\n",
        "rowSubplot = 4\n",
        "colSubplot = 3\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "imgBGR = cv2.imread(\"HW/g3/rgb/136.jpg\")\n",
        "grayimg = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2GRAY)\n",
        "minVal,maxVal,_,_ = cv2.minMaxLoc(grayimg)\n",
        "plt.subplot(rowSubplot,colSubplot,1)\n",
        "plt.imshow(grayimg,cmap='gray')\n",
        "\n",
        "res=grayimg\n",
        "cv2.convertScaleAbs(grayimg,res,255.0/(maxVal-minVal),-minVal)\n",
        "plt.subplot(rowSubplot,colSubplot,2)\n",
        "plt.imshow(res,cmap='gray')\n",
        "\n",
        "imgHSV = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2HSV);\n",
        "hue_channel = imgHSV[:,:,0]\n",
        "saturation_channel = imgHSV[:,:,1]\n",
        "value_channel = imgHSV[:,:,2]\n",
        "plt.subplot(rowSubplot,colSubplot,4)\n",
        "plt.imshow(hue_channel,cmap='gray')\n",
        "plt.subplot(rowSubplot,colSubplot,5)\n",
        "plt.imshow(saturation_channel,cmap='gray')\n",
        "plt.subplot(rowSubplot,colSubplot,6)\n",
        "plt.imshow(value_channel,cmap='gray')\n",
        "\n",
        "avg_hue = np.average(hue_channel)\n",
        "hue_channel_thresh, hue_channel_segment = cv2.threshold(hue_channel,avg_hue,255,cv2.THRESH_BINARY)\n",
        "plt.subplot(rowSubplot,colSubplot,10)\n",
        "plt.imshow(hue_channel_segment,cmap='gray')\n",
        "\n",
        "avg_saturation = np.average(saturation_channel)\n",
        "saturation_channel_thresh, saturation_channel_segment = cv2.threshold(saturation_channel,(avg_saturation)/2,255,cv2.THRESH_BINARY)\n",
        "plt.subplot(rowSubplot,colSubplot,11)\n",
        "plt.imshow(saturation_channel_segment+hue_channel_segment,cmap='gray')\n",
        "\n",
        "blue_channel = imgBGR[:,:,0]\n",
        "green_channel = imgBGR[:,:,1]\n",
        "red_channel = imgBGR[:,:,2]\n",
        "plt.subplot(rowSubplot,colSubplot,7)\n",
        "plt.imshow(blue_channel,cmap='gray')\n",
        "plt.subplot(rowSubplot,colSubplot,8)\n",
        "plt.imshow(green_channel,cmap='gray')\n",
        "plt.subplot(rowSubplot,colSubplot,9)\n",
        "plt.imshow(red_channel,cmap='gray')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL7_ps4RZ0rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all subfolders in a directory\n",
        "import os\n",
        "myFolderList = [f.path for f in os.scandir(path) if f.is_dir()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vj_xm0najIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all files with extension in a directory\n",
        "import glob\n",
        "import re\n",
        " \n",
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "    return sorted(l, key = alphanum_key)\n",
        " \n",
        "names = sorted_nicely(glob.glob1(path, \"*.extension\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYQvzVjJaulE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class names\n",
        "classNames = ['traffic sign', 'vehicle', 'cactus']\n",
        "subclassNames = [\n",
        "    ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking', 'Road works', 'Roundabout', 'Slippery road', 'Stop', 'Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)', 'Wild animals', 'X - Priority', 'X - Turn left', 'X - Turn right'],\n",
        "    ['SUV','truck','plane'],\n",
        "    ['happy','sad','angry','evil']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gxeCxDPazgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the first images\n",
        "colors = [(0,0,255),(255,0,255),(0,255,0)]\n",
        "\n",
        "def drawBBs(BBs, img):\n",
        "    img = cv2.resize(img, (1280, 960))\n",
        "    for BB in BBs:\n",
        "        u = BB[0]*2\n",
        "        v = BB[1]*2\n",
        "        w = BB[2]*2\n",
        "        h = BB[3]*2\n",
        "        c = BB[4]\n",
        "        sc = BB[5]\n",
        "        x = BB[6]\n",
        "        y = BB[7]\n",
        "        z = BB[8]\n",
        "        s = (u - w // 2, v - h // 2)\n",
        "        e = (u + w // 2, v + h // 2)\n",
        "        cv2.rectangle(img, s, e, colors[c], 1)\n",
        "        tl = (s[0], s[1]+15)\n",
        "        bl = (s[0], e[1]-5)\n",
        "        cv2.putText(img,subclassNames[c][sc],tl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.75,colors[c])\n",
        "        coords = \"(%.2f, %.2f, %.2f)\" % (x,y,z)\n",
        "        cv2.putText(img,coords,bl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.65,colors[c])\n",
        "    \n",
        "    return img\n",
        "\n",
        "import pickle\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "#This way it doesn't try to open a window un the GUI - works in python notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Read images\n",
        "img = cv2.imread(\"HW/g1/rgb/1.jpg\")\n",
        "depth = cv2.imread(\"HW/g1/depth/1.png\", -1)\n",
        "\n",
        "# Read annotations\n",
        "file = open('HW/annotations.pickle','rb')\n",
        "annotations = pickle.load(file)\n",
        "\n",
        "# Visualization\n",
        "depth = depth / 5000.0\n",
        "img = drawBBs(annotations[\"HW/g1/rgb/1.jpg\"][\"objects\"], img)\n",
        "img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Figure with subplots\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_rgb)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(depth,cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjRbWeZbkppp",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXNe5h1-kszB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "\n",
        "from torch import optim\n",
        "from torchvision import transforms\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "haveCuda = torch.cuda.is_available()\n",
        "numEpoch = 2\n",
        "batch_size = 32\n",
        "num_workers = 4\n",
        "\n",
        "classNames = ['traffic sign', 'vehicle', 'cactus']\n",
        "subclassNames = [\n",
        "    ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking', 'Road works', 'Roundabout', 'Slippery road', 'Stop', 'Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)', 'Wild animals', 'X - Priority', 'X - Turn left', 'X - Turn right'],\n",
        "    ['SUV','truck','plane'],\n",
        "    ['happy','sad','angry','evil']\n",
        "]\n",
        "\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "\n",
        "def transformData():\n",
        "\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.49139968, 0.48215827, 0.44653124),    # atlagok   -> ez nem kell\n",
        "                            (0.24703233, 0.24348505, 0.26158768))    # szorasok\n",
        "    ])\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),   # random shift\n",
        "        transforms.RandomHorizontalFlip(),      # Flips horizontally with p=0.5\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.49139968, 0.48215827, 0.44653124),\n",
        "                            (0.24703233, 0.24348505, 0.26158768))\n",
        "    ])\n",
        "\n",
        "    return transform, transform_val\n",
        "\n",
        "\n",
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "    return sorted(l, key = alphanum_key)\n",
        "\n",
        "\n",
        "class trafficSignSet(Dataset):\n",
        "    def __init__(self, manifest_filepath, labels, augment=True, transform=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param manifest_filepath: Path to manifest csv as describe above\n",
        "        :param labels: String containing all the possible characters to map to\n",
        "        :param augment(default False):  Apply data augmentation\n",
        "\n",
        "        \"\"\"\n",
        "        myFolderList = [f.path for f in os.scandir('./trafficSignsHW/trainFULL') if f.is_dir()]\n",
        "        myFolderList.sort()\n",
        "\n",
        "        for i in myFolderList:\n",
        "          names = sorted_nicely(glob.glob1(i, \"*.jpg\"))\n",
        "          for j in names:\n",
        "            ids = [i][i,j]\n",
        "\n",
        "        self.ids = ids\n",
        "        self.size = len(ids)\n",
        "        super(SpectrogramDataset, self).__init__(audio_conf, normalize, augment)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return sample = self.ids[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    def func(p):\n",
        "        return p[0].size(1)\n",
        "\n",
        "    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
        "    longest_sample = max(batch, key=func)[0]\n",
        "    minibatch_size = len(batch)\n",
        "    inputs = torch.zeros(minibatch_size)\n",
        "\n",
        "    for x in range(minibatch_size):\n",
        "        sample = batch[x]\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "class trafficSignLoader(DataLoader):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(trafficSignLoader, self).__init__(*args, **kwargs)\n",
        "        self.collate_fn = _collate_fn\n",
        "\n",
        "    \n",
        "class Conv(nn.Module):\n",
        "  \n",
        "    def __init__(self, in_channels, channels, k_size=3, stride=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels, channels, k_size, stride=stride, padding=k_size//2, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(channels)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        return self.bn(torch.relu(self.conv(x)))\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, base_channels=16, in_channels=3, num_classes=55):  # 16\n",
        "        super().__init__()\n",
        "        \n",
        "        self.c11 = Conv(in_channels, base_channels)\n",
        "        self.c12 = Conv(base_channels, base_channels)\n",
        "        self.d1  = Conv(base_channels, base_channels*2, stride=2) # Downscale using strided convolution and expand channels\n",
        "        \n",
        "        self.c21 = Conv(base_channels*2, base_channels*2)\n",
        "        self.c22 = Conv(base_channels*2, base_channels*2)\n",
        "        self.d2  = Conv(base_channels*2, base_channels*4, stride=2)\n",
        "        \n",
        "        self.c31 = Conv(base_channels*4, base_channels*4)\n",
        "        self.c32 = Conv(base_channels*4, base_channels*4)\n",
        "        self.d3  = Conv(base_channels*4, base_channels*8, stride=2)\n",
        "        \n",
        "        self.c41 = Conv(base_channels*8, base_channels*8)\n",
        "        self.c42 = Conv(base_channels*8, base_channels*8)\n",
        "        self.d4  = Conv(base_channels*8, base_channels*16, stride=2)\n",
        "        \n",
        "        self.c51 = Conv(base_channels*16, base_channels*16)\n",
        "        self.c52 = Conv(base_channels*16, base_channels*16)\n",
        "        self.d5  = Conv(base_channels*16, base_channels*32, stride=2) \n",
        "\n",
        "        # Input image is 32x32 -> after 5 downscaling the activation map is 1x1\n",
        "        # [batch, ch, h, w]  -> [batch, ch] linearisba, h=1, w=1\n",
        "        # Classifier is a normal 1x1 convolution that produces num_classes class scores\n",
        "        # This layer does not have BatchNorm of ReLU\n",
        "\n",
        "        self.classifier = nn.Conv2d(base_channels*32, num_classes, kernel_size=1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.d1(self.c12(self.c11(x)))\n",
        "        x = self.d2(self.c22(self.c21(x)))\n",
        "        x = self.d3(self.c32(self.c31(x)))\n",
        "        x = self.d4(self.c42(self.c41(x)))\n",
        "        x = self.d5(self.c52(self.c51(x)))\n",
        "        \n",
        "        return torch.squeeze(self.classifier(x))    # After squeeze is becomes (batch_size x num_classes)\n",
        "\n",
        "\n",
        "def train(epoch, trainLoader):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    bar = display(progress(0, len(trainLoader)), display_id=True)     # Create progress bar\n",
        "\n",
        "    for i, data in enumerate(trainLoader, 0):\n",
        "\n",
        "        inputs, labels = data\n",
        "        if haveCuda:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            running_loss += loss.item()     # Accumulate loss\n",
        "            _, predicted = torch.max(outputs, 1)  # [batch, num_class] -> [batch, 1]\n",
        "            correct += predicted.eq(labels).sum().item() # Count how many of the predictions equal the labels\n",
        "            total += labels.shape[0]   # Accumulate number of total images seen\n",
        "\n",
        "        bar.update(progress(i+1, len(trainLoader)))\n",
        "\n",
        "    tr_loss = running_loss / i\n",
        "    tr_corr = correct / total * 100\n",
        "    print(\"Train epoch %d loss: %.3f correct: %.2f\" % (epoch + 1, running_loss / i, tr_corr))\n",
        "\n",
        "    return tr_loss,tr_corr\n",
        "\n",
        "\n",
        "def val(epoch,testLoader):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    bar = display(progress(0, len(testLoader)), display_id=True)\n",
        "\n",
        "    for i, data in enumerate(testLoader, 0):\n",
        "\n",
        "        inputs, labels = data\n",
        "        if haveCuda:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.shape[0]\n",
        "\n",
        "        bar.update(progress(i+1, len(testLoader)))\n",
        "\n",
        "    val_loss = running_loss / i\n",
        "    val_corr = correct / total * 100\n",
        "    print(\"Test epoch %d loss: %.3f correct: %.2f\" % (epoch + 1, running_loss / i, val_corr))\n",
        "\n",
        "    return val_loss,val_corr\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    transform, transform_val = transformData()\n",
        "\n",
        "    train_dataset = trafficSignSet(manifest_filepath='./trafficSignsHW/trainFULL', \n",
        "                                   labels=subclassNames[0],\n",
        "                                   transform=transform\n",
        "                                   )\n",
        "    test_dataset = trafficSignSet(manifest_filepath='./trafficSignsHW/testFULL', \n",
        "                                  labels=subclassNames[0],\n",
        "                                  transform=transform_val\n",
        "                                  )\n",
        "                          \n",
        "    train_loader = trafficSignLoader(train_dataset, \n",
        "                                     shuffle=True, \n",
        "                                     batch_size=batch_size,\n",
        "                                     num_workers=num_workers\n",
        "                                     )\n",
        "    test_loader = trafficSignLoader(test_dataset, \n",
        "                                    batch_size=batch_size,\n",
        "                                    num_workers=num_workers\n",
        "                                    )\n",
        "\n",
        "    train_accs = []\n",
        "    train_losses = []\n",
        "    val_accs = []\n",
        "    val_losses = []\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    torch.manual_seed(1)  # Set pseudo-random generator seeds to make multiple runs comparable\n",
        "    if haveCuda:\n",
        "        torch.cuda.manual_seed(1)\n",
        "\n",
        "    net = ConvNet(4)\n",
        "    if haveCuda:\n",
        "        net = net.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=1e-1, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,numEpoch,eta_min=1e-2)\n",
        "\n",
        "    for epoch in range(numEpoch):\n",
        "        \n",
        "        loss,acc = train(epoch,train_loader)\n",
        "        train_accs.append(acc)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        loss,acc = val(epoch,test_loader)\n",
        "        val_accs.append(acc)\n",
        "        val_losses.append(loss)\n",
        "\n",
        "        scheduler.step()\n",
        "      \n",
        "        if acc > best_acc:\n",
        "            print(\"Best Model, Saving\")\n",
        "            best_acc = acc\n",
        "            torch.save(net,\"./model/pyVision.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGZIbHC_mgLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# X coordinate for plotting\n",
        "x = np.arange(numEpoch)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# Train is red, validation is blue\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(x,train_accs,'r')\n",
        "plt.plot(x,val_accs,'b')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(x,train_losses,'r')\n",
        "plt.plot(x,val_losses,'b')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRVJOM4smg_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a minibatch from the test loader and convert to cuda\n",
        "inputs, labels = next(iter(testLoader))\n",
        "if haveCuda:\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "# forward\n",
        "outputs = net(inputs)\n",
        "\n",
        "# Get predicted class indices\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Values used for normalization\n",
        "mean = torch.Tensor((0.49139968, 0.48215827, 0.44653124)).unsqueeze(1).unsqueeze(1)\n",
        "std = torch.Tensor((0.24703233, 0.24348505, 0.26158768)).unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "# Class names\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# List of subplots - we'll use 16 images\n",
        "f, axarr = plt.subplots(2, 8,figsize=(20, 5))\n",
        "\n",
        "# For every image-prediction pair\n",
        "for i,(img,pred) in enumerate(zip(inputs,predicted)):\n",
        "    # undo the normalization\n",
        "    img_rescaled = img.cpu() * std + mean\n",
        "    \n",
        "    # Get predicted class name\n",
        "    name = classes[pred.cpu().item()]\n",
        "    \n",
        "    # Permutation needed because in PyTorch the channel dimension comes first,\n",
        "    # but in numpy and opencv it comes last (3x32x32) -> (32x32x3)\n",
        "    axarr[i//8,i%8].imshow(img_rescaled.permute(1,2,0))\n",
        "    \n",
        "    # Set title to class name\n",
        "    axarr[i//8,i%8].set_title(name)\n",
        "    \n",
        "    # Hide grid lines\n",
        "    axarr[i//8,i%8].grid(False)\n",
        "    \n",
        "    # Hide axes ticks\n",
        "    axarr[i//8,i%8].set_xticks([])\n",
        "    axarr[i//8,i%8].set_yticks([])\n",
        "    \n",
        "    # Only do the first 16\n",
        "    if i == 15:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}